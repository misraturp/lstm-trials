{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 20, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0     After waiting what seemed like  G3   forever ...\n1     Yeah   G10   it was such a change I  G9   swo...\n2       G8  And I was so glad that all of the measu...\n3     Ben and I had been saying for the past two we...\n4     Of course   G13  that is what happened for th...\nName: sentence, dtype: object"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "_PADDING_VALUE = 4"
        }, 
        {
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def process_sentences(sentence):\n    \n    #make all words lowercase\n    sentence_lower = sentence.lower()\n    \n    #string to list of words\n    sentence_separated = sentence_lower.split()\n    \n    #remove non alphabetic words\n    sentence_alphabetic = [word for word in sentence_separated if word.isalpha() or re.search(r'g\\d{1,2}',word) is not None]\n\n    #pad the sentence with four 0s\n    sentence_padded = ['0']*_PADDING_VALUE+sentence_alphabetic\n    \n    processed_sentence = np.array(sentence_padded)\n    return sentence_alphabetic    "
        }, 
        {
            "execution_count": 23, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#df to list\nsentences_list = df_data.values.tolist()\n\nprocessed_sentences = [process_sentences(sent[0]) for sent in sentences_list]"
        }, 
        {
            "execution_count": 24, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Total Sequences: 18667\n"
                }
            ], 
            "source": "length = _PADDING_VALUE+1\nsequences = list()\nfor sentence in processed_sentences:\n    for i in range(length, len(sentence)):\n        #select the sequence of words\n        seq = sentence[i-length:i]\n        # convert into a line\n        line = ' '.join(str(v) for v in seq)\n        #store\n        sequences.append(line)\nprint('Total Sequences: %d' % len(sequences))"
        }, 
        {
            "execution_count": 25, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 25, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "['after waiting what seemed like',\n 'waiting what seemed like g3',\n 'what seemed like g3 forever',\n 'seemed like g3 forever to',\n 'like g3 forever to get',\n 'g3 forever to get into',\n 'forever to get into the',\n 'to get into the room',\n 'get into the room we',\n 'into the room we g19']"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "sequences[:10]"
        }, 
        {
            "execution_count": 26, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n"
                }
            ], 
            "source": "from numpy import array\nfrom pickle import dump\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding"
        }, 
        {
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#integer encode sequences of words\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(sequences)\nnumerical_sequences = tokenizer.texts_to_sequences(sequences)"
        }, 
        {
            "execution_count": 28, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 28, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "2982"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# vocabulary size\nvocab_size = len(tokenizer.word_index) + 1\nvocab_size"
        }, 
        {
            "execution_count": 30, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# separate into input and output\nnumerical_sequences = array(numerical_sequences)\nX, y = numerical_sequences[:,:-1], numerical_sequences[:,-1]\ny = to_categorical(y, num_classes=vocab_size)\nseq_length = X.shape[1]"
        }, 
        {
            "execution_count": 34, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 4, 4)              11928     \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 4, 100)            42000     \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 100)               80400     \n_________________________________________________________________\ndense_1 (Dense)              (None, 100)               10100     \n_________________________________________________________________\ndense_2 (Dense)              (None, 2982)              301182    \n=================================================================\nTotal params: 445,610\nTrainable params: 445,610\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
                }
            ], 
            "source": "# define model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 4, input_length=seq_length))\nmodel.add(LSTM(100, return_sequences=True))\nmodel.add(LSTM(100))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(vocab_size, activation='softmax'))\nprint(model.summary())"
        }, 
        {
            "execution_count": 35, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Epoch 1/100\n18667/18667 [==============================] - 11s - loss: 6.8491 - acc: 0.0448    \nEpoch 2/100\n18667/18667 [==============================] - 11s - loss: 6.3680 - acc: 0.0494    \nEpoch 3/100\n18667/18667 [==============================] - 10s - loss: 6.3314 - acc: 0.0494    \nEpoch 4/100\n18667/18667 [==============================] - 10s - loss: 6.3060 - acc: 0.0494    \nEpoch 5/100\n18667/18667 [==============================] - 10s - loss: 6.2904 - acc: 0.0494    \nEpoch 6/100\n18667/18667 [==============================] - 10s - loss: 6.2793 - acc: 0.0481    \nEpoch 7/100\n18667/18667 [==============================] - 10s - loss: 6.2579 - acc: 0.0496    \nEpoch 8/100\n18667/18667 [==============================] - 10s - loss: 6.2410 - acc: 0.0496    \nEpoch 9/100\n18667/18667 [==============================] - 10s - loss: 6.2292 - acc: 0.0494    \nEpoch 10/100\n18667/18667 [==============================] - 10s - loss: 6.2115 - acc: 0.0498    \nEpoch 11/100\n18667/18667 [==============================] - 10s - loss: 6.1876 - acc: 0.0499    \nEpoch 12/100\n18667/18667 [==============================] - 11s - loss: 6.1613 - acc: 0.0502    \nEpoch 13/100\n18667/18667 [==============================] - 10s - loss: 6.1215 - acc: 0.0505    \nEpoch 14/100\n18667/18667 [==============================] - 10s - loss: 6.0461 - acc: 0.0497    \nEpoch 15/100\n18667/18667 [==============================] - 10s - loss: 5.9276 - acc: 0.0547    \nEpoch 16/100\n18667/18667 [==============================] - 10s - loss: 5.7864 - acc: 0.0626    \nEpoch 17/100\n18667/18667 [==============================] - 11s - loss: 5.6844 - acc: 0.0647    \nEpoch 18/100\n18667/18667 [==============================] - 10s - loss: 5.5759 - acc: 0.0697    \nEpoch 19/100\n18667/18667 [==============================] - 10s - loss: 5.4776 - acc: 0.0715    \nEpoch 20/100\n18667/18667 [==============================] - 10s - loss: 5.3904 - acc: 0.0762    \nEpoch 21/100\n18667/18667 [==============================] - 10s - loss: 5.3013 - acc: 0.0784    \nEpoch 22/100\n18667/18667 [==============================] - 10s - loss: 5.2087 - acc: 0.0818    \nEpoch 23/100\n18667/18667 [==============================] - 10s - loss: 5.1173 - acc: 0.0853    \nEpoch 24/100\n18667/18667 [==============================] - 11s - loss: 5.0234 - acc: 0.0880    \nEpoch 25/100\n18667/18667 [==============================] - 10s - loss: 4.9376 - acc: 0.0916    \nEpoch 26/100\n18667/18667 [==============================] - 10s - loss: 4.8599 - acc: 0.0924    \nEpoch 27/100\n18667/18667 [==============================] - 10s - loss: 4.7773 - acc: 0.0978    \nEpoch 28/100\n18667/18667 [==============================] - 10s - loss: 4.7008 - acc: 0.0991    \nEpoch 29/100\n18667/18667 [==============================] - 10s - loss: 4.6198 - acc: 0.1051    \nEpoch 30/100\n18667/18667 [==============================] - 10s - loss: 4.5486 - acc: 0.1083    \nEpoch 31/100\n18667/18667 [==============================] - 10s - loss: 4.4724 - acc: 0.1154    \nEpoch 32/100\n18667/18667 [==============================] - 11s - loss: 4.4022 - acc: 0.1199    \nEpoch 33/100\n18667/18667 [==============================] - 11s - loss: 4.3362 - acc: 0.1250    \nEpoch 34/100\n18667/18667 [==============================] - 11s - loss: 4.2664 - acc: 0.1289    \nEpoch 35/100\n18667/18667 [==============================] - 11s - loss: 4.1919 - acc: 0.1370    \nEpoch 36/100\n18667/18667 [==============================] - 11s - loss: 4.1268 - acc: 0.1465    \nEpoch 37/100\n18667/18667 [==============================] - 11s - loss: 4.0574 - acc: 0.1475    \nEpoch 38/100\n18667/18667 [==============================] - 11s - loss: 3.9946 - acc: 0.1614    \nEpoch 39/100\n18667/18667 [==============================] - 11s - loss: 3.9240 - acc: 0.1674    \nEpoch 40/100\n18667/18667 [==============================] - 10s - loss: 3.8685 - acc: 0.1730    \nEpoch 41/100\n18667/18667 [==============================] - 11s - loss: 3.8019 - acc: 0.1801    \nEpoch 42/100\n18667/18667 [==============================] - 10s - loss: 3.7373 - acc: 0.1919    \nEpoch 43/100\n18667/18667 [==============================] - 10s - loss: 3.6775 - acc: 0.1997    \nEpoch 44/100\n18667/18667 [==============================] - 11s - loss: 3.6212 - acc: 0.2069    \nEpoch 45/100\n18667/18667 [==============================] - 11s - loss: 3.5480 - acc: 0.2194    \nEpoch 46/100\n18667/18667 [==============================] - 11s - loss: 3.5017 - acc: 0.2299    \nEpoch 47/100\n18667/18667 [==============================] - 11s - loss: 3.4343 - acc: 0.2388    \nEpoch 48/100\n18667/18667 [==============================] - 11s - loss: 3.3832 - acc: 0.2491    \nEpoch 49/100\n18667/18667 [==============================] - 11s - loss: 3.3350 - acc: 0.2559    \nEpoch 50/100\n18667/18667 [==============================] - 11s - loss: 3.2768 - acc: 0.2689    \nEpoch 51/100\n18667/18667 [==============================] - 11s - loss: 3.2208 - acc: 0.2759    \nEpoch 52/100\n18667/18667 [==============================] - 11s - loss: 3.1692 - acc: 0.2884    \nEpoch 53/100\n18667/18667 [==============================] - 11s - loss: 3.1167 - acc: 0.2949    \nEpoch 54/100\n18667/18667 [==============================] - 11s - loss: 3.0750 - acc: 0.3030    \nEpoch 55/100\n18667/18667 [==============================] - 11s - loss: 3.0424 - acc: 0.3060    \nEpoch 56/100\n18667/18667 [==============================] - 10s - loss: 2.9878 - acc: 0.3182    \nEpoch 57/100\n18667/18667 [==============================] - 11s - loss: 2.9416 - acc: 0.3282    \nEpoch 58/100\n18667/18667 [==============================] - 11s - loss: 2.8972 - acc: 0.3355    \nEpoch 59/100\n18667/18667 [==============================] - 11s - loss: 2.8595 - acc: 0.3444    \nEpoch 60/100\n18667/18667 [==============================] - 10s - loss: 2.8170 - acc: 0.3525    \nEpoch 61/100\n18667/18667 [==============================] - 11s - loss: 2.7889 - acc: 0.3561    \nEpoch 62/100\n18667/18667 [==============================] - 11s - loss: 2.7442 - acc: 0.3635    \nEpoch 63/100\n18667/18667 [==============================] - 10s - loss: 2.7156 - acc: 0.3714    \nEpoch 64/100\n18667/18667 [==============================] - 11s - loss: 2.6872 - acc: 0.3740    \nEpoch 65/100\n18667/18667 [==============================] - 11s - loss: 2.6464 - acc: 0.3849    \nEpoch 66/100\n18667/18667 [==============================] - 11s - loss: 2.6175 - acc: 0.3889    \nEpoch 67/100\n18667/18667 [==============================] - 11s - loss: 2.5794 - acc: 0.4018    \nEpoch 68/100\n18667/18667 [==============================] - 11s - loss: 2.5579 - acc: 0.4011    \nEpoch 69/100\n18667/18667 [==============================] - 10s - loss: 2.5249 - acc: 0.4099    \nEpoch 70/100\n18667/18667 [==============================] - 11s - loss: 2.4932 - acc: 0.4139    \nEpoch 71/100\n18667/18667 [==============================] - 11s - loss: 2.4667 - acc: 0.4181    \nEpoch 72/100\n18667/18667 [==============================] - 11s - loss: 2.4327 - acc: 0.4272    \nEpoch 73/100\n18667/18667 [==============================] - 11s - loss: 2.4125 - acc: 0.4311    \nEpoch 74/100\n18667/18667 [==============================] - 10s - loss: 2.3850 - acc: 0.4371    \nEpoch 75/100\n18667/18667 [==============================] - 10s - loss: 2.3606 - acc: 0.4413    \nEpoch 76/100\n18667/18667 [==============================] - 11s - loss: 2.3302 - acc: 0.4486    \nEpoch 77/100\n18667/18667 [==============================] - 11s - loss: 2.3150 - acc: 0.4547    \nEpoch 78/100\n18667/18667 [==============================] - 11s - loss: 2.2863 - acc: 0.4554    \nEpoch 79/100\n18667/18667 [==============================] - 11s - loss: 2.2580 - acc: 0.4643    \nEpoch 80/100\n18667/18667 [==============================] - 11s - loss: 2.2409 - acc: 0.4646    \nEpoch 81/100\n18667/18667 [==============================] - 11s - loss: 2.2143 - acc: 0.4736    \nEpoch 82/100\n18667/18667 [==============================] - 11s - loss: 2.1989 - acc: 0.4733    \nEpoch 83/100\n18667/18667 [==============================] - 11s - loss: 2.1780 - acc: 0.4792    \nEpoch 84/100\n18667/18667 [==============================] - 11s - loss: 2.1596 - acc: 0.4842    \nEpoch 85/100\n18667/18667 [==============================] - 11s - loss: 2.1294 - acc: 0.4912    \nEpoch 86/100\n18667/18667 [==============================] - 11s - loss: 2.1086 - acc: 0.4925    \nEpoch 87/100\n18667/18667 [==============================] - 11s - loss: 2.0883 - acc: 0.4960    \nEpoch 88/100\n18667/18667 [==============================] - 11s - loss: 2.0756 - acc: 0.5010    \nEpoch 89/100\n18667/18667 [==============================] - 11s - loss: 2.0603 - acc: 0.5013    \nEpoch 90/100\n18667/18667 [==============================] - 11s - loss: 2.0354 - acc: 0.5100    \nEpoch 91/100\n18667/18667 [==============================] - 11s - loss: 2.0255 - acc: 0.5094    \nEpoch 92/100\n18667/18667 [==============================] - 11s - loss: 2.0014 - acc: 0.5186    \nEpoch 93/100\n18667/18667 [==============================] - 11s - loss: 1.9792 - acc: 0.5195    \nEpoch 94/100\n18667/18667 [==============================] - 11s - loss: 1.9633 - acc: 0.5261    \nEpoch 95/100\n18667/18667 [==============================] - 11s - loss: 1.9465 - acc: 0.5262    \nEpoch 96/100\n18667/18667 [==============================] - 11s - loss: 1.9283 - acc: 0.5340    \nEpoch 97/100\n18667/18667 [==============================] - 11s - loss: 1.9112 - acc: 0.5354    \nEpoch 98/100\n18667/18667 [==============================] - 11s - loss: 1.8881 - acc: 0.5432    \nEpoch 99/100\n18667/18667 [==============================] - 11s - loss: 1.8773 - acc: 0.5433    \nEpoch 100/100\n18667/18667 [==============================] - 11s - loss: 1.8598 - acc: 0.5458    \n"
                }, 
                {
                    "execution_count": 35, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "<keras.callbacks.History at 0x7f38881959b0>"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# compile model\ncreate_model = model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nfit_model = model.fit(X, y, batch_size=128, epochs=100)"
        }, 
        {
            "execution_count": 115, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "the person you want\n\n"
                }
            ], 
            "source": "whole_sentence = 'the person you want to meet will be downstairs in a second .'\n#select a seed text\n#seed_text = lines[randint(0,len(lines))]\nseed_text = 'the person you want'\nprint(seed_text + '\\n')"
        }, 
        {
            "execution_count": 119, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# generate a sequence from a language model\ndef generate_seq(model, tokenizer, seq_length, seed_text, whole_sentence, n_words):\n    result = [seed_text]\n    count = seq_length\n    in_text = seed_text\n    whole_sentence = whole_sentence.split()\n    # generate a fixed number of words\n    for _ in range(seq_length, len(whole_sentence)):\n        # encode the text as integer\n        encoded = tokenizer.texts_to_sequences([in_text])[0]\n        # truncate sequences to a fixed length\n        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n        # predict probabilities for each word\n        yhat = model.predict_classes(encoded, verbose=0)\n        # map predicted word index to word\n        out_word = ''\n        for word, index in tokenizer.word_index.items():\n            if index == yhat:\n                out_word = word\n                break\n        print(out_word)\n        if re.search(r'g\\d{1,2}',out_word) is not None:\n            # append to input\n            in_text += ' ' + out_word\n            result.append(out_word)\n        else:\n            next_word = whole_sentence[count]\n            in_text += ' ' + next_word\n            result.append(next_word)\n            count += 1\n    return ' '.join(result)"
        }, 
        {
            "execution_count": 120, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "perhaps\nget\nyou\ng14\na\nnothing\nnothing\nthrough\npeas\n"
                }
            ], 
            "source": "resulting_sentence = generate_seq(model, tokenizer, seq_length, seed_text, whole_sentence, 10)"
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "NameError", 
                    "evalue": "name 'resulting_sentence' is not defined", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-1-c66cd6987b64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresulting_sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0m: name 'resulting_sentence' is not defined"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "resulting_sentence"
        }, 
        {
            "execution_count": 132, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "model.h5\r\n"
                }
            ], 
            "source": "!ls '../LSTM'"
        }, 
        {
            "execution_count": 131, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# save the model to file\nmodel.save('../LSTM/model.h5')"
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "service_path = \"https://ibm-watson-ml.eu-gb.bluemix.net\"\nusername = \"cf3630f1-60f6-424e-9e28-069fbc198b86\"\npassword = \"f6db3b3e-422f-46f8-a702-054b095e489a\"\ninstance_id = \"76b51224-c4a5-4ebf-8bd3-ad26309ea5a5\""
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import urllib3, requests, json\n\nheaders = urllib3.util.make_headers(basic_auth='{}:{}'.format(username, password))\nurl = '{}/v3/identity/token'.format(service_path)\nresponse = requests.get(url, headers=headers)\nmltoken = 'Bearer ' + json.loads(response.text).get('token')"
        }, 
        {
            "execution_count": 24, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "<Response [200]>\n{\"metadata\":{\"guid\":\"76b51224-c4a5-4ebf-8bd3-ad26309ea5a5\",\"url\":\"https://instances/v3/wml_instances/76b51224-c4a5-4ebf-8bd3-ad26309ea5a5\",\"created_at\":\"2018-05-14T09:57:26.088Z\",\"modified_at\":\"2018-05-14T09:57:26.088Z\"},\"entity\":{\"source\":\"Bluemix\",\"published_models\":{\"url\":\"https://ibm-watson-ml.eu-gb.bluemix.net/v3/wml_instances/76b51224-c4a5-4ebf-8bd3-ad26309ea5a5/published_models\"},\"usage\":{\"expiration_date\":\"2018-06-01T00:00:00.000Z\",\"computation_time\":{\"limit\":180000,\"current\":0},\"model_count\":{\"limit\":200,\"current\":0},\"prediction_count\":{\"limit\":5000,\"current\":0},\"gpu_count\":{\"limit\":8,\"current\":0},\"capacity_units\":{\"limit\":180000000,\"current\":0},\"deployment_count\":{\"limit\":5,\"current\":0}},\"plan_id\":\"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe\",\"status\":\"Active\",\"organization_guid\":\"812f63ef-afd1-4d70-b6ee-c549b869eb09\",\"region\":\"eu-gb\",\"account\":{\"id\":\"c8e459ba42cd1088b0badad838a0b2e5\",\"name\":\"vu amsetrdam\",\"type\":\"TRIAL\"},\"owner\":{\"ibm_id\":\"50W8DA7BU3\",\"email\":\"m.turp@student.vu.nl\",\"user_id\":\"7d7eb296-7e2c-400d-85d9-862c1d72a659\",\"country_code\":\"NLD\",\"beta_user\":false},\"deployments\":{\"url\":\"https://ibm-watson-ml.eu-gb.bluemix.net/v3/wml_instances/76b51224-c4a5-4ebf-8bd3-ad26309ea5a5/deployments\"},\"space_guid\":\"4d0a9449-582e-4969-b285-1e42424fca34\",\"plan\":\"lite\"}}\n"
                }
            ], 
            "source": "endpoint_instance = service_path + \"/v3/wml_instances/\" + instance_id\nheader = {'Content-Type': 'application/json', 'Authorization': mltoken}\n\nresponse_get_instance = requests.get(endpoint_instance, headers=header)\nprint(response_get_instance)\nprint(response_get_instance.text)"
        }, 
        {
            "execution_count": 26, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "https://ibm-watson-ml.eu-gb.bluemix.net/v3/wml_instances/76b51224-c4a5-4ebf-8bd3-ad26309ea5a5/published_models\n"
                }
            ], 
            "source": "endpoint_published_models = json.loads(response_get_instance.text).get('entity').get('published_models').get('url')\nprint(endpoint_published_models)"
        }, 
        {
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "<Response [200]>\n{\"limit\":1000,\"resources\":[],\"first\":{\"url\":\"https://deployment/v3/wml_instances/76b51224-c4a5-4ebf-8bd3-ad26309ea5a5/published_models?limit=1000\"}}\n"
                }
            ], 
            "source": "header = {'Content-Type': 'application/json', 'Authorization': mltoken}\n\nresponse_get = requests.get(endpoint_published_models, headers=header)\nprint(response_get)\nprint(response_get.text)"
        }, 
        {
            "execution_count": 28, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "ValueError", 
                    "evalue": "not enough values to unpack (expected 1, got 0)", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-28-156349a0a717>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m [endpoint_deployments] = [x.get('entity').get('deployments').get('url') \n\u001b[0;32m----> 2\u001b[0;31m for x in json.loads(response_get.text).get('resources') if x.get('metadata').get('guid') == saved_model.uid]\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_deployments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 1, got 0)"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "[endpoint_deployments] = [x.get('entity').get('deployments').get('url') \nfor x in json.loads(response_get.text).get('resources') if x.get('metadata').get('guid') == saved_model.uid]\n\nprint(endpoint_deployments)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}