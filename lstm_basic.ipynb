{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": 111, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 111, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0     After waiting what seemed like  G52   forever...\n1     Yeah   G15   it was such a change! I  G40   s...\n2       G13  And I was so glad that all of the meas...\n3     Ben and I had been saying for the past two we...\n4     Of course   G20  that is what happened for th...\nName: sentence, dtype: object"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 47, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "['after', 'waiting', 'what', 'seemed', 'like', 'g52', 'forever', 'to', 'get', 'into', 'the', 'room', '.', 'we', 'g28', 'finally', 'got', 'to', 'see', 'our', 'little', 'one', 'for', 'the', 'first', 'time', 'in', '7', 'weeks', 'g15', 'g44', 'and', 'wow', 'what', 'a', 'change', 'it', 'was', '!']\ntraining_data length: 874\nlongest sentence: 160\nshortest sentence: 1\navrg sentence length: 29.1830663616\nstandard deviation of sentence length: 17.4226866681\nsentence length: 47.0\n"
                }
            ], 
            "source": "\nclass data():\n    def __init__(self):\n        self.data = []\n        \nclass mini_data():\n    def __init__(self):\n        self.mini_data = []\n        \nd = data()\n\n'''separate words in sentences and store them in lists'''\ndef read_data(sentence):\n    md = mini_data()\n    sentence = sentence.lower()\n    #content = sentence.split()\n    content = re.findall(r\"[\\w']+|[.,!?;]\", sentence)\n    new = md.mini_data + content\n    d.data.append(new)\n\ndf_data['sentence'].apply(read_data)\ntraining_data = np.array(d.data)\nprint(training_data[0])\n\nprint('training_data length: ' + str(len(training_data)))\n\navrgs = [len(x) for x in training_data]\nprint('longest sentence: ' + str(np.max(avrgs)))\nprint('shortest sentence: ' + str(np.min(avrgs)))\nprint('avrg sentence length: ' + str(np.mean(avrgs)))\nprint('standard deviation of sentence length: ' + str(np.std(avrgs)))\n\n'''sentence length will be the average length plus the standard deviation'''\nsentence_length = round(np.mean(avrgs) + np.std(avrgs))\nprint('sentence length: ' + str(sentence_length))\n"
        }, 
        {
            "execution_count": 139, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "3214\n"
                }
            ], 
            "source": "import collections\n\ndef filter_gestures(word):\n    ges = re.search(r'g\\d{1,2}',word)\n    if ges is None:\n        return True\n    else:\n        return False\n\ndef build_dictionary(words):\n    count = collections.Counter(words).most_common()\n    dictionary = dict()\n    for word, _ in count:\n        dictionary[word] = len(dictionary)+1\n    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n    return dictionary, reverse_dictionary\n\n'''make the list of list of words into list of words'''\nwords = [item for sublist in training_data for item in sublist]\n\n'''filter the gesture annotations'''\nwords = [x for x in words if filter_gestures(x)]\n\n'''count the unique appearance of words and assign them a number'''\ndictionary, reverse_dictionary = build_dictionary(words)\n\n#print(dictionary)\nvocab_size = len(dictionary)\nprint(vocab_size)"
        }, 
        {
            "execution_count": 134, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import tensorflow as tf\nfrom tensorflow.contrib import rnn\n\n#sequence length\ntime_steps=4\n\n#could be any number\nnum_units=20\n\n#self explanatory\ninput_size=1\n\n#we need to learn about this\nlearning_rate=0.001\n\n#57 getsure types + no gesture\nn_classes=58\nno_gesture_index = 57\n\n#tbd\nbatch_size=100"
        }, 
        {
            "execution_count": 110, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "25506\n25503\n25503\n[259, 1, 1, 1]\n"
                }
            ], 
            "source": "\ntrain_input = []\ntrain_output = []\n\ndef create_input_output(flat_data):\n    \n    prev_gesture = False\n    \n    single_input = []\n    single_output = [0]*n_classes    \n    \n    for i in range(len(flat_data)):\n    \n        count = 0\n        extra_index = 0\n        words = flat_data[i+extra_index:i+time_steps+extra_index]\n        \n        if len(words) != time_steps:\n            break\n        \n        while count < time_steps + extra_index:\n            \n            word = words[count]   \n            gesture = re.search(r'g\\d{1,2}', word)\n\n            if gesture is None:\n\n                num = dictionary[word]\n                single_input.append(num)\n                \n                if count+1 == len(words) and sum(single_output) == 0:\n                    single_output[no_gesture_index] = 1\n\n            elif gesture is not None:\n\n                if count+1 == len(words):\n                    index_gesture = int(word[1:])\n                    single_output[index_gesture] = 1\n                \n                new_word = flat_data[i+time_steps+extra_index]\n                words.append(new_word)\n                extra_index = extra_index + 1\n                \n            count = count+1\n                \n            \n        train_input.append(single_input)\n        train_output.append(single_output)  \n\n        single_input = []\n        single_output = [0]*n_classes  \n    \n    return 'Done!'\n            \nflat_training_data = [item for sublist in training_data for item in sublist]\ncreate_input_output(flat_training_data)\n                       \nprint(len(flat_training_data))                 \nprint(len(train_input))\nprint(len(train_output))"
        }, 
        {
            "execution_count": 128, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "20500\n5003\n"
                }
            ], 
            "source": "#Random sampling training and test data\n\n_TRAINING_INS_COUNT = 20500\ntrain_indexes = random.sample(range(0, len(train_input)), _TRAINING_INS_COUNT)\ntest_indexes = [i for i in range(0, len(train_input)) if not i in train_indexes]\n\n\n#creating training data\n_TRAINING_INPUT = np.array(train_input)\n_TRAINING_INPUT = _TRAINING_INPUT[train_indexes]\n_TRAINING_OUTPUT = np.array(train_output)\n_TRAINING_OUTPUT = _TRAINING_OUTPUT[train_indexes]\n\n\n#creating testing data\n_TESTING_INPUT = np.array(train_input)\n_TESTING_INPUT = _TESTING_INPUT[test_indexes]\n_TESTING_OUTPUT = np.array(train_output)\n_TESTING_OUTPUT = _TESTING_OUTPUT[test_indexes]\n"
        }, 
        {
            "execution_count": 130, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Tensor(\"Placeholder_2:0\", shape=(?, 4, 1), dtype=float32)\n[<tf.Tensor 'unstack_1:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack_1:1' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack_1:2' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack_1:3' shape=(?, 1) dtype=float32>]\n"
                }
            ], 
            "source": "#weights and biases of appropriate shape to accomplish above task\nout_weights=tf.Variable(tf.random_normal([num_units,n_classes]))\nout_bias=tf.Variable(tf.random_normal([n_classes]))\n\n#defining placeholders\n#input image placeholder\nx=tf.placeholder(\"float\",[None,time_steps,input_size])\nprint(x)\n#input label placeholder\ny=tf.placeholder(\"float\",[None,n_classes])\n\n#processing the input tensor from [batch_size,n_steps,input_size] to \"time_steps\" number of [batch_size,input_size] tensors\ninput=tf.unstack(x ,time_steps,1)\nprint(input)"
        }, 
        {
            "execution_count": 131, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Tensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 20), dtype=float32)\nTensor(\"rnn/add:0\", shape=(?, 58), dtype=float32)\n"
                }
            ], 
            "source": "# defining the network\nlstm_layer=rnn.BasicLSTMCell(num_units,forget_bias=1)\n\nwith tf.variable_scope(\"rnn\", reuse=None):\n    outputs,_=rnn.static_rnn(lstm_layer,input,dtype=\"float32\")\n    print(outputs[-1])\n    \n    #converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\n    prediction=tf.matmul(outputs[-1],out_weights)+out_bias\n    print(prediction)\n\n    #loss_function\n    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n    #optimization\n    opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n\n    #model evaluationmatmul\n    correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n    accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
        }, 
        {
            "execution_count": 137, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Tensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 20), dtype=float32)\nFor iter  10\nAccuracy  0.01\nLoss  3.04814\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 20), dtype=float32)\nFor iter  20\nAccuracy  0.3\nLoss  2.62893\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 20), dtype=float32)\nFor iter  30\nAccuracy  0.64\nLoss  1.84877\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 20), dtype=float32)\nFor iter  40\nAccuracy  0.86\nLoss  1.52903\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 20), dtype=float32)\nFor iter  50\nAccuracy  0.9\nLoss  1.03908\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 20), dtype=float32)\nFor iter  60\nAccuracy  0.93\nLoss  0.775554\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 20), dtype=float32)\nFor iter  70\nAccuracy  0.87\nLoss  1.36215\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 20), dtype=float32)\nFor iter  80\nAccuracy  0.93\nLoss  0.712309\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 20), dtype=float32)\nFor iter  90\nAccuracy  0.96\nLoss  0.531634\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 20), dtype=float32)\nFor iter  100\nAccuracy  0.92\nLoss  0.960766\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 20), dtype=float32)\nFor iter  110\nAccuracy  0.94\nLoss  0.697692\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 20), dtype=float32)\nFor iter  120\nAccuracy  0.94\nLoss  0.703309\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 20), dtype=float32)\nFor iter  130\nAccuracy  0.96\nLoss  0.409456\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 20), dtype=float32)\nFor iter  140\nAccuracy  0.93\nLoss  0.714086\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 20), dtype=float32)\nFor iter  150\nAccuracy  0.94\nLoss  0.573838\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 20), dtype=float32)\nFor iter  160\nAccuracy  0.89\nLoss  1.01804\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 20), dtype=float32)\nFor iter  170\nAccuracy  0.92\nLoss  0.885654\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 20), dtype=float32)\nFor iter  180\nAccuracy  0.95\nLoss  0.503991\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 20), dtype=float32)\nFor iter  190\nAccuracy  0.91\nLoss  0.751503\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 20), dtype=float32)\nFor iter  200\nAccuracy  0.95\nLoss  0.541904\n__________________\nTesting Accuracy: 0.91665\n"
                }
            ], 
            "source": "#initialize variables\nbatch_init = 0\nlimit = batch_init + batch_size\ninit=tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    \n    sess.run(init)\n    iter=1\n    while iter<205:\n        \n        batch_x = _TRAINING_INPUT[batch_init: limit]\n        batch_y = _TRAINING_OUTPUT[batch_init: limit]\n        batch_init = batch_init + batch_size\n        limit = batch_init + batch_size\n\n        batch_x=batch_x.reshape((batch_size,time_steps,input_size))\n\n        sess.run(opt, feed_dict={x: batch_x, y: batch_y})\n\n        if iter %10==0:\n            print(outputs[-1])\n            acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})\n            los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})\n            print(\"For iter \",iter)\n            print(\"Accuracy \",acc)\n            print(\"Loss \",los)\n            print(\"__________________\")\n\n        iter=iter+1\n        \n    #calculating test accuracy\n    test_data = _TESTING_INPUT.reshape((-1, time_steps, input_size))\n    test_label = _TESTING_OUTPUT\n    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: test_data, y: test_label}))\n    #print(\"Prediction:\", sess.run(prediction, feed_dict={x: test_data, y: test_label}))\n    #d = prediction.eval(feed_dict={x: test_data}, session=sess)\n    #print (\"predictions\", d)\n    #print(type(d[0]))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}