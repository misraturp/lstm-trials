{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": 159, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "mkdir: cannot create directory \u2018../datasets\u2019: File exists\r\n"
                }
            ], 
            "source": "!mkdir \"../datasets\"\n!mkdir \"../datasets/LSTM\""
        }, 
        {
            "execution_count": 131, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "mkdir: cannot create directory \u2018../TT_data\u2019: File exists\r\n"
                }
            ], 
            "source": "!mkdir \"../TT_data\""
        }, 
        {
            "execution_count": 148, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\n!ls '../datasets'\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#LATEST VERSION: ONLY INCLUDE THE WORD GROUPS THAT HAVE A GESTURE (NO NON-GESTURE CLASSIFIED WORD SEQUENCES)\n#DOES NORMAL CLASSIFICTAION(NOT WEIGHTED), NO MULTIPLE LAYERS"
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 1, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0     After waiting what seemed like  G52   forever...\n1     Yeah   G15   it was such a change! I  G40   s...\n2       G13  And I was so glad that all of the meas...\n3     Ben and I had been saying for the past two we...\n4     Of course   G20  that is what happened for th...\nName: sentence, dtype: object"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "['after', 'waiting', 'what', 'seemed', 'like', 'g52', 'forever', 'to', 'get', 'into', 'the', 'room', '.', 'we', 'g28', 'finally', 'got', 'to', 'see', 'our', 'little', 'one', 'for', 'the', 'first', 'time', 'in', '7', 'weeks', 'g15', 'g44', 'and', 'wow', 'what', 'a', 'change', 'it', 'was', '!']\ntraining_data length: 874\nlongest sentence: 160\nshortest sentence: 1\navrg sentence length: 29.1830663616\nstandard deviation of sentence length: 17.4226866681\nsentence length: 47.0\n"
                }
            ], 
            "source": "\nclass data():\n    def __init__(self):\n        self.data = []\n        \nclass mini_data():\n    def __init__(self):\n        self.mini_data = []\n        \nd = data()\n\n'''separate words in sentences and store them in lists'''\ndef read_data(sentence):\n    md = mini_data()\n    sentence = sentence.lower()\n    #content = sentence.split()\n    content = re.findall(r\"[\\w']+|[.,!?;]\", sentence)\n    new = md.mini_data + content\n    d.data.append(new)\n    \ndf_data['sentence'].apply(read_data)\ntraining_data = np.array(d.data)\nprint(training_data[0])\n\nprint('training_data length: ' + str(len(training_data)))\n\navrgs = [len(x) for x in training_data]\nprint('longest sentence: ' + str(np.max(avrgs)))\nprint('shortest sentence: ' + str(np.min(avrgs)))\nprint('avrg sentence length: ' + str(np.mean(avrgs)))\nprint('standard deviation of sentence length: ' + str(np.std(avrgs)))\n\n'''sentence length will be the average length plus the standard deviation'''\nsentence_length = round(np.mean(avrgs) + np.std(avrgs))\nprint('sentence length: ' + str(sentence_length))\n"
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "3214\n"
                }
            ], 
            "source": "import collections\n\ndef filter_gestures(word):\n    ges = re.search(r'g\\d{1,2}',word)\n    if ges is None:\n        return True\n    else:\n        return False\n\ndef build_dictionary(words):\n    count = collections.Counter(words).most_common()\n    dictionary = dict()\n    for word, _ in count:\n        dictionary[word] = len(dictionary)+1\n    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n    return dictionary, reverse_dictionary\n\n'''make the list of list of words into list of words'''\nwords = [item for sublist in training_data for item in sublist]\n\n'''filter the gesture annotations'''\nwords = [x for x in words if filter_gestures(x)]\n\n'''count the unique appearance of words and assign them a number'''\ndictionary, reverse_dictionary = build_dictionary(words)\n\n#print(dictionary)\nvocab_size = len(dictionary)\nprint(vocab_size)\n"
        }, 
        {
            "execution_count": 316, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dict_df = pd.DataFrame(list(dictionary.items()))\ndict_df.to_csv('../TT_data/dictionary.csv', index=False)\n\nrevdict_df = pd.DataFrame(list(reverse_dictionary.items()))\nrevdict_df.to_csv('../TT_data/rev_dictionary.csv', index=False)"
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import tensorflow as tf\nfrom tensorflow.contrib import rnn\n\n#sequence length\ntime_steps=4\n\n#could be any number\nnum_units=10\n\n#self explanatory\ninput_size=1\n\n#we need to learn about this\nlearning_rate=0.001\n\n#57 getsure types + no gesture\nn_classes=58\nno_gesture_index = 57\n\n#tbd\nbatch_size=50\n"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "25506\n25503\n25503\n"
                }
            ], 
            "source": "\ndef create_input_output(flat_data):\n    \n    #initialize variables\n    train_input = []\n    train_output = []\n    \n    single_input = []\n    single_output = [0]*n_classes    \n    \n    #'''for every word in the data set'''\n    for i in range(len(flat_data)):\n    \n        count = 0\n        extra_index = 0\n        \n        #'''get them in groups of four'''\n        words = flat_data[i+extra_index:i+time_steps+extra_index]\n        \n        #'''if we have reached the end of the dataset end this function'''\n        if len(words) != time_steps:\n            break\n        \n        #'''until the single_output is full'''\n        while len(single_input) < time_steps:\n            \n            word = words[count] \n            gesture = re.search(r'g\\d{1,2}', word)\n\n            #'''if the word being analysed is not a gesture'''\n            if gesture is None:\n\n                #'''find correspondig value in dictionary and add to input'''\n                num = dictionary[word]\n                single_input.append(num)\n                \n\n            #'''if the word being analysed is a gesture'''\n            elif gesture is not None:\n            \n                \n                #'''get the next word from the flatlist and add it to the word to be analysed array'''\n                new_word = flat_data[i+time_steps+extra_index]\n                words.append(new_word)\n                extra_index = extra_index + 1\n               \n            \n            #'''when the input array is full'''\n            if len(single_input) == time_steps:\n                \n                gesture_check = None\n                \n                #'''get the next entry in the flat list and check if it's a gesture'''\n                if i+time_steps+extra_index < len(flat_data):\n                    next_entry = flat_data[i+time_steps+extra_index]\n                    gesture_check = re.search(r'g\\d{1,2}', next_entry)\n                \n                #'''if yes, mark that gesture in the output of this particular input'''\n                if gesture_check is not None:\n                    index_gesture = int(next_entry[1:])\n                    single_output[index_gesture] = 1\n                    \n                #'''if no, mark the no gesture cell in the output array'''\n                else:\n                    single_output[no_gesture_index] = 1\n                    \n                \n            count = count+1\n                \n            \n        train_input.append(single_input)\n        train_output.append(single_output)  \n\n        single_input = []\n        single_output = [0]*n_classes  \n    \n    return train_input, train_output\n            \n#'''make list from list of lists'''\nflat_training_data = [item for sublist in training_data for item in sublist]\n\n#'''populate the input and output data'''\ntrain_input, train_output = create_input_output(flat_training_data)\n                       \nprint(len(flat_training_data))                 \nprint(len(train_input))\nprint(len(train_output))"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 8, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "2210"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "def get_indexes_to_be_filtered(list):\n    \n    indices = []\n    \n    for i in range(0,len(list)):\n        if list[i][no_gesture_index]==1:\n            indices.append(i)\n        \n    return indices\n\nindices = get_indexes_to_be_filtered(train_output)\ntrain_actual_output = [train_output[i] for i in range(0, len(train_output)) if i not in indices]\nlen(train_actual_output)\n\ntrain_actual_input = [train_input[i] for i in range(0, len(train_input)) if i not in indices]\nlen(train_actual_input)\n\n    "
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[30, 59, 27, 17, 94, 55, 27, 63, 97, 95, 14, 26, 45, 40, 27, 41, 34, 30, 61, 16, 49, 71, 46, 50, 42, 101, 21, 20, 37, 34, 7, 38, 18, 35, 99, 109, 23, 52, 27, 15, 12, 13, 17, 19, 28, 21, 14, 31, 15, 25, 22, 10, 19, 24, 123, 6, 19, 23293]\n"
                }
            ], 
            "source": "#creating the weight array for the weighted loss function\n\ncounts = [item.index(1) for item in train_output]\ncount_total = [counts.count(i) for i in range(0,58)]\nprint(count_total)\n\nweight_array_1 = [1 - (item/sum(count_total)) for item in count_total]\n#last_we = weight_array_1[no_gesture_index]\n#weight_array_1 = [0.99]*n_classes\n#weight_array_1[no_gesture_index] = last_we\n#weight_array_1\n"
        }, 
        {
            "execution_count": 385, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 385, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[0.01752322224217171,\n 0.017503272748785655,\n 0.01752528598286682,\n 0.017532165118517185,\n 0.017479195774009384,\n 0.017506024403045804,\n 0.01752528598286682,\n 0.01750052109452551,\n 0.017477132033314274,\n 0.017478507860444345,\n 0.017534228859212294,\n 0.017525973896431856,\n 0.017512903538696164,\n 0.017516343106521347,\n 0.01752528598286682,\n 0.017515655192956312,\n 0.017520470587911566,\n 0.01752322224217171,\n 0.017501896921655585,\n 0.01753285303208222,\n 0.01751015188443602,\n 0.01749501778600522,\n 0.01751221562513113,\n 0.017509463970870984,\n 0.017514967279391273,\n 0.01747438037905413,\n 0.01752941346425704,\n 0.017530101377822075,\n 0.017518406847216457,\n 0.017520470587911566,\n 0.01753904425416755,\n 0.01751771893365142,\n 0.01753147720495215,\n 0.01751978267434653,\n 0.0174757562061842,\n 0.017468877070533836,\n 0.017528037637126966,\n 0.01750808814374091,\n 0.01752528598286682,\n 0.01753354094564726,\n 0.017535604686342368,\n 0.01753491677277733,\n 0.017532165118517185,\n 0.01753078929138711,\n 0.017524598069301782,\n 0.01752941346425704,\n 0.017534228859212294,\n 0.017522534328606676,\n 0.01753354094564726,\n 0.01752666180999689,\n 0.017528725550692,\n 0.01753698051347244,\n 0.01753078929138711,\n 0.01752734972356193,\n 0.017459246280623328,\n 0.017539732167732584,\n 0.01753078929138711,\n 0.0015202889787303997]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "weight_array_2 = [weight/sum(weight_array_1) for weight in weight_array_1]\nweight_array_2"
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 15, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "110"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "#Random sampling training and test data\n_TRAINING_INS_COUNT = 2100\ntrain_indexes = random.sample(range(0, len(train_actual_input)), _TRAINING_INS_COUNT)\ntest_indexes = [i for i in range(0, len(train_actual_input)) if not i in train_indexes]\n\n\n#creating training data 80%\n_TRAINING_INPUT = np.array(train_actual_input)\n_TRAINING_INPUT = _TRAINING_INPUT[train_indexes]\n_TRAINING_OUTPUT = np.array(train_actual_output)\n_TRAINING_OUTPUT = _TRAINING_OUTPUT[train_indexes]\n\n\n#creating testing data 20%\n_TESTING_INPUT = np.array(train_actual_input)\n_TESTING_INPUT = _TESTING_INPUT[test_indexes]\n_TESTING_OUTPUT = np.array(train_actual_output)\n_TESTING_OUTPUT = _TESTING_OUTPUT[test_indexes]\n\nlen(_TESTING_INPUT)\n\n"
        }, 
        {
            "execution_count": 388, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "testin = pd.DataFrame(_TESTING_INPUT)\ntestout = pd.DataFrame(_TESTING_OUTPUT)\ntestin.to_csv(\"../TT_data/testing_input.csv\", index=False)\ntestout.to_csv(\"../TT_data/testing_output.csv\", index=False)"
        }, 
        {
            "execution_count": 389, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "dictionary.csv\trev_dictionary.csv  testing_input.csv  testing_output.csv\r\n"
                }
            ], 
            "source": "!ls '../TT_data'"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Tensor(\"Placeholder:0\", shape=(?, 4, 1), dtype=float32)\n[<tf.Tensor 'unstack:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:1' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:2' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:3' shape=(?, 1) dtype=float32>]\n"
                }
            ], 
            "source": "tf.reset_default_graph()\n\n#weights and biases of appropriate shape to accomplish above task\nout_weights=tf.Variable(tf.random_normal([num_units,n_classes]), name=\"weights\")\nout_bias=tf.Variable(tf.random_normal([n_classes]), name=\"biases\")\n\n#defining placeholders\n#input image placeholder\nx=tf.placeholder(\"float\",[None,time_steps,input_size])\nprint(x)\n#input label placeholder\ny=tf.placeholder(\"float\",[None,n_classes])\n\n#processing the input tensor from [batch_size,n_steps,input_size] to \"time_steps\" number of [batch_size,input_size] tensors\ninput=tf.unstack(x ,time_steps,1)\nprint(input)"
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# defining the network\nlstm_layer=rnn.BasicLSTMCell(num_units,forget_bias=1)\n\nwith tf.variable_scope(\"rnn\", reuse=None):\n    outputs,state=rnn.static_rnn(lstm_layer,input,dtype=\"float32\")\n        \n    #converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\n    prediction=tf.matmul(outputs[-1],out_weights)+out_bias\n\n    #loss_function\n    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n    #optimization\n    opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n\n    #model evaluationmatmul\n    correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n    accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 10), dtype=float32)\nFor iter  10\nAccuracy  0.0\nLoss  4.88057\n__________________\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 10), dtype=float32)\nFor iter  20\nAccuracy  0.04\nLoss  4.82415\n__________________\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 10), dtype=float32)\nFor iter  30\nAccuracy  0.02\nLoss  4.64039\n__________________\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_11:0\", shape=(?, 10), dtype=float32)\nFor iter  40\nAccuracy  0.02\nLoss  4.48269\n__________________\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n../datasets/LSTM_1/model.ckpt\nTesting Accuracy: 0.0454545\n"
                }
            ], 
            "source": "#initialize variables\nbatch_init = 0\nlimit = batch_init + batch_size\ninit=tf.global_variables_initializer()\nsaver = tf.train.Saver()\n\nwith tf.Session() as sess:\n    \n    sess.run(init)\n    iter=1\n    while iter<42:\n        \n        batch_x = _TRAINING_INPUT[batch_init: limit]\n        batch_y = _TRAINING_OUTPUT[batch_init: limit]\n        batch_init = batch_init + batch_size\n        limit = batch_init + batch_size\n\n        batch_x=batch_x.reshape((batch_size,time_steps,input_size))\n\n        sess.run(opt, feed_dict={x: batch_x, y: batch_y})\n        \n        raw_predictions = prediction.eval(feed_dict={x: batch_x}, session=sess).tolist()\n        results = [sublist.index(max(sublist)) for sublist in raw_predictions]\n        print(results)\n\n        if iter %10==0:\n            print(outputs[-1])\n            acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})\n            los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})\n            print(\"For iter \",iter)\n            print(\"Accuracy \",acc)\n            print(\"Loss \",los)\n            print(\"__________________\")\n\n        iter=iter+1        \n    \n    save_path = saver.save(sess,\"../datasets/LSTM_1/model.ckpt\")\n    print(save_path)\n\n    #calculating test accuracy\n    test_data = _TESTING_INPUT.reshape((-1, time_steps, input_size))\n    test_label = _TESTING_OUTPUT\n    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: test_data, y: test_label}))\n    \n    #print(\"Prediction:\", sess.run(prediction, feed_dict={x: test_data, y: test_label}))\n    #d = prediction.eval(feed_dict={x: test_data}, session=sess)\n    #print (\"predictions\", d)\n    #print(type(d[0]))\n    \n    #YOU PREPARED THE DATA, YOU WERE TRYING TO MAKE THE LAST BIT WORK"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!ls \"../datasets/LSTM\""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# after this point the next two cells are downloading the prediction & actual output files\n#i do not need them anymore but I am keeping them there just to make sure I have the code ready in case I need it"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}