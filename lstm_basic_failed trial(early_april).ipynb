{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": 69, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 69, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0     After waiting what seemed like  G52   forever...\n1     Yeah   G15   it was such a change I  G40   sw...\n2       G13  And I was so glad that all of the meas...\n3     Ben and I had been saying for the past two we...\n4     Of course   G20  that is what happened for th...\nName: sentence, dtype: object"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 70, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "['ever', 'since', 'we', 'were', 'g33', 'g5', 'five', 'our', 'dads', 'take', 'us', 'camping', 'and', 'let', 'me', 'tell', 'yall', 'whose', 'tent', 'was', 'the', 'brightest', 'g11', 'tent', 'ever', 'thats', 'right', 'mine', 'it', 'even', 'out', 'shone', 'henrys', 'but', 'g34', 'thats', 'so', 'you', 'know', 'wed', 'have', 'a', 'point', 'of', 'g6', 'reference', 'if', 'any', 'of', 'us', 'got', 'lost', 'in', 'the', 'night', 'times', 'when', 'there', 'are', 'scary', 'things', 'around', 'and', 'the', 'portapotty', 'seems', 'like', 'a', 'million', 'g56', 'miles', 'away']\ntraining_data length: 874\nlongest sentence: 150\nshortest sentence: 1\navrg sentence length: 27.0778032037\nstandard deviation of sentence length: 16.6001126717\nsentence length: 44.0\n"
                }
            ], 
            "source": "\nclass data():\n    def __init__(self):\n        self.data = []\n        \nclass mini_data():\n    def __init__(self):\n        self.mini_data = []\n        \nd = data()\n\n'''separate words in sentences and store them in lists'''\ndef read_data(sentence):\n    md = mini_data()\n    sentence = sentence.lower()\n    content = sentence.split()\n    new = md.mini_data + content\n    d.data.append(new)\n\ndf_data['sentence'].apply(read_data)\ntraining_data = np.array(d.data)\nprint(training_data[38])\n\nprint('training_data length: ' + str(len(training_data)))\n\navrgs = [len(x) for x in training_data]\nprint('longest sentence: ' + str(np.max(avrgs)))\nprint('shortest sentence: ' + str(np.min(avrgs)))\nprint('avrg sentence length: ' + str(np.mean(avrgs)))\nprint('standard deviation of sentence length: ' + str(np.std(avrgs)))\n\n'''sentence length will be the average length plus the standard deviation'''\nsentence_length = round(np.mean(avrgs) + np.std(avrgs))\nprint('sentence length: ' + str(sentence_length))\n"
        }, 
        {
            "execution_count": 71, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "any\n3220\n"
                }
            ], 
            "source": "import collections\n\ndef filter_gestures(word):\n    ges = re.search(r'g\\d{1,2}',word)\n    if ges is None:\n        return True\n    else:\n        return False\n\ndef build_dictionary(words):\n    count = collections.Counter(words).most_common()\n    dictionary = dict()\n    for word, _ in count:\n        dictionary[word] = len(dictionary)\n    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n    return dictionary, reverse_dictionary\n\n'''make the list of list of words into list of words'''\nwords = [item for sublist in training_data for item in sublist]\n\n'''filter the gesture annotations'''\nwords = [x for x in words if filter_gestures(x)]\n\n'''count the unique appearance of words and assign them a number'''\ndictionary, reverse_dictionary = build_dictionary(words)\n\nprint(reverse_dictionary[183])\nvocab_size = len(dictionary)\nprint(vocab_size)"
        }, 
        {
            "execution_count": 72, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "874\n44\n874\n45\n[235, 124, 5, 15, 582, 25, 1836, 134, 66, 312, 1, 438, 42, 465, 739, 2535, 680, 7, 0, 2412, 680, 235, 120, 95, 638, 9, 71, 26, 2955, 3107, 21, 120, 11, 37, 52, 549, 22, 4, 276, 6, 1892, 69, 183, 6]\n[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n44.0\n"
                }
            ], 
            "source": "\ntrain_input = []\ntrain_output = []\n\ndef create_input_output(sentence):\n    \n    prev_gesture = False\n    \n    '''create a 44-length array'''\n    single_input = [-1]*int(sentence_length)\n    single_output = [0]*(int(sentence_length)+1)\n    loc = 0\n    sentence_iterator = 0\n    \n    while loc < int(sentence_length):\n\n        if sentence_iterator < len(sentence):\n            word = sentence[sentence_iterator]\n            gesture = re.search(r'g\\d{1,2}',word)\n\n            if gesture is None:\n\n                prev_gesture = False\n\n                num_word = dictionary[word]\n\n                single_input[loc] = num_word\n                loc = loc + 1\n                sentence_iterator = sentence_iterator +1\n\n            else:                    \n                if not prev_gesture:\n                    num_gesture = int(word[1:])\n                    del single_input[loc]\n                    single_input.append(-1)\n                    single_output[loc] = 1\n                    prev_gesture = True\n                sentence_iterator = sentence_iterator +1\n        else:\n            break\n                \n                \n    train_input.append(single_input)\n    train_output.append(single_output)\n    return 'Done!'\n        \n#create_input_output(training_data[0])\nret_value = [create_input_output(sentence) for sentence in training_data]\nprint(len(train_input))\nprint(len(train_input[356]))\nprint(len(train_output))\nprint(len(train_output[38]))\n\nprint(train_input[38])\nprint(train_output[38])\nprint(sentence_length)"
        }, 
        {
            "execution_count": 73, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "45\n"
                }
            ], 
            "source": "\nNUM_EXAMPLES = 800\ntest_input = np.array(train_input[NUM_EXAMPLES:])\ntest_output = train_output[NUM_EXAMPLES:] #everything beyond 800\nprint(len(test_output[0]))\n \ntrain_input = np.array(train_input[:NUM_EXAMPLES])\ntrain_output = train_output[:NUM_EXAMPLES] #till 800\n"
        }, 
        {
            "execution_count": 74, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import tensorflow as tf\nfrom tensorflow.contrib import rnn\n\n#define constants\n#unrolled through 28 time steps\ntime_steps=11\n#hidden LSTM units\nnum_units=50\n#rows of 28 pixels\nn_input=4\n#learning rate for adam\nlearning_rate=0.001\n#mnist is meant to be classified in 10 classes(0-9).\nn_classes=45\n#size of batch\nbatch_size=5"
        }, 
        {
            "execution_count": 75, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#weights and biases of appropriate shape to accomplish above task\nout_weights=tf.Variable(tf.random_normal([num_units,n_classes]))\nout_bias=tf.Variable(tf.random_normal([n_classes]))\n\n#defining placeholders\n#input image placeholder\nx=tf.placeholder(\"float\",[None,time_steps,n_input])\n#input label placeholder\ny=tf.placeholder(\"float\",[None,n_classes])\n\n#processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size,n_input] tensors\ninput=tf.unstack(x ,time_steps,1)"
        }, 
        {
            "execution_count": 76, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# defining the network\nlstm_layer=rnn.BasicLSTMCell(num_units,forget_bias=1)\n\nwith tf.variable_scope(\"rnn\", reuse=None):\n    outputs,_=rnn.static_rnn(lstm_layer,input,dtype=\"float32\")\n    #converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\n    prediction=tf.matmul(outputs[-1],out_weights)+out_bias\n\n    #loss_function\n    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n    #optimization\n    opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n\n    #model evaluationmatmul\n    correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n    accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
        }, 
        {
            "execution_count": 77, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Tensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_32:0\", shape=(?, 50), dtype=float32)\nFor iter  10\nAccuracy  0.0\nLoss  10.6534\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_32:0\", shape=(?, 50), dtype=float32)\nFor iter  20\nAccuracy  0.0\nLoss  6.98304\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_32:0\", shape=(?, 50), dtype=float32)\nFor iter  30\nAccuracy  0.2\nLoss  10.4772\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_32:0\", shape=(?, 50), dtype=float32)\nFor iter  40\nAccuracy  0.0\nLoss  14.4043\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_32:0\", shape=(?, 50), dtype=float32)\nFor iter  50\nAccuracy  0.2\nLoss  8.45381\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_32:0\", shape=(?, 50), dtype=float32)\nFor iter  60\nAccuracy  0.2\nLoss  15.0665\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_32:0\", shape=(?, 50), dtype=float32)\nFor iter  70\nAccuracy  0.0\nLoss  14.5159\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_32:0\", shape=(?, 50), dtype=float32)\nFor iter  80\nAccuracy  0.4\nLoss  9.11083\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_32:0\", shape=(?, 50), dtype=float32)\nFor iter  90\nAccuracy  0.4\nLoss  8.89609\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_32:0\", shape=(?, 50), dtype=float32)\nFor iter  100\nAccuracy  0.2\nLoss  17.0348\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_32:0\", shape=(?, 50), dtype=float32)\nFor iter  110\nAccuracy  0.4\nLoss  14.7236\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_32:0\", shape=(?, 50), dtype=float32)\nFor iter  120\nAccuracy  0.0\nLoss  17.6979\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_32:0\", shape=(?, 50), dtype=float32)\nFor iter  130\nAccuracy  0.2\nLoss  11.291\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_32:0\", shape=(?, 50), dtype=float32)\nFor iter  140\nAccuracy  0.0\nLoss  21.7812\n__________________\nTensor(\"rnn/rnn/rnn/basic_lstm_cell/mul_32:0\", shape=(?, 50), dtype=float32)\nFor iter  150\nAccuracy  0.2\nLoss  12.3107\n__________________\nTesting Accuracy: 0.108108\n"
                }
            ], 
            "source": "#initialize variables\nbatch_init = 0\nlimit = batch_init + batch_size\ninit=tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    \n    sess.run(init)\n    iter=1\n    while iter<160:\n        \n        batch_x = train_input[batch_init: limit]\n        batch_y = train_output[batch_init: limit]\n        batch_init = batch_init + batch_size\n        limit = batch_init + batch_size\n\n        batch_x=batch_x.reshape((batch_size,time_steps,n_input))\n\n        sess.run(opt, feed_dict={x: batch_x, y: batch_y})\n\n        if iter %10==0:\n            print(outputs[-1])\n            acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})\n            los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})\n            print(\"For iter \",iter)\n            print(\"Accuracy \",acc)\n            print(\"Loss \",los)\n            print(\"__________________\")\n\n        iter=iter+1\n        \n    #calculating test accuracy\n    test_data = test_input[:74].reshape((-1, time_steps, n_input))\n    test_label = test_output[:74]\n    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: test_data, y: test_label}))\n    #print(\"Prediction:\", sess.run(prediction, feed_dict={x: test_data, y: test_label}))\n    #d = prediction.eval(feed_dict={x: test_data}, session=sess)\n    #print (\"predictions\", d)\n    #print(type(d[0]))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}
